{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eebc93f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textblob import TextBlob\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "#from gensim.models import word2vec\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#nltk.download('popular', halt_on_error=False)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from sklearn import metrics\n",
    "#textblob\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "# Import Counter\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import words as eng_words\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68c8466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Takes a random post from this subreddit when there's no plates.\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('T1_train.csv')\n",
    "\n",
    "data.head()\n",
    "\n",
    "y = data[\"label\"]\n",
    "X = data.loc[:,~data.columns.isin([\"label\"])]\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42,stratify = y)\n",
    "\n",
    "\n",
    "print(X_train.iloc[7,:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d1a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    " #Create a function to get the polarity\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "def punkt_frew_nor(x):\n",
    "    try:\n",
    "        count = 0\n",
    "        leng = len(x)\n",
    "        for i in x:\n",
    "            if i in string.punctuation:\n",
    "                count+=1\n",
    "        return count/leng\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def upper_word_freq(x):\n",
    "    try:\n",
    "        count = 0\n",
    "        leng = len(x)\n",
    "        for i in x.split(\" \"):\n",
    "            if i.isupper():\n",
    "                count+=1\n",
    "        return count/leng\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "subjectivity_train_x    = X_train['text'].apply(getSubjectivity)\n",
    "subjectivity_test_x     = X_test['text'].apply(getSubjectivity)\n",
    "\n",
    "polarity_train_x        = X_train['text'].apply(getPolarity)\n",
    "polarity_test_x         = X_test['text'].apply(getPolarity)\n",
    "\n",
    "\n",
    "\n",
    "upper_word_freq_train_x = X_train['text'].apply(upper_word_freq)\n",
    "upper_word_freq_test_x  = X_test['text'].apply(upper_word_freq)\n",
    "\n",
    "\n",
    "\n",
    "punkt_frew_nor_train_x  = X_train['text'].apply(punkt_frew_nor)\n",
    "punkt_frew_nor_test_x   = X_test['text'].apply(punkt_frew_nor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bb2615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"take a random post from this subreddit when there 's no plate\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def parse(s):\n",
    "    s = s.replace(\"cant\",r\"can't\").replace(\"wheres\",r\"where's\").replace(\" i \",\" I \")\n",
    "    return str(s[0].upper()) + s[1:]\n",
    "\n",
    "def clean_text(texts):\n",
    "    #TextBlos is object which helps to get words and lemmatize them,remove punctuaions and singularize words\n",
    "    try:\n",
    "        text=TextBlob(parse(texts))\n",
    "        #spell = SpellChecker()\n",
    "        words=text.words\n",
    "        sw = stopwords.words('english')\n",
    "        #[spell.correction(i)  if i.islower() else i for i in words]\n",
    "        new = [i.lower() for i in words if i.lower()]\n",
    "        cleaned=TextBlob(\" \".join(new))\n",
    "        a=[]\n",
    "        ## here I will loop twice to lemmatize both verbs and nouns\n",
    "        for i in cleaned.words:\n",
    "            a.append(i.lemmatize())\n",
    "        c=TextBlob(\" \".join(a)).words\n",
    "        return \" \".join(c)\n",
    "    except:\n",
    "        return texts\n",
    "\n",
    " #I did so because while fitting model on the data error occured demanding string type\n",
    "\n",
    "    \n",
    "example=\"\"\"Takes a random post from this subreddit when there's no :) plates.\"\"\"\n",
    "clean_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86484b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-cc60bcebe89f>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['text']=X_train['text'].apply(clean_text)# applying functon on a column\n",
      "<ipython-input-5-cc60bcebe89f>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['text']=X_test['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "X_train['text']=X_train['text'].apply(clean_text)# applying functon on a column\n",
    "X_test['text']=X_test['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c594807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_range=(1,3)\n",
    "\n",
    "# data['text'] = data['text'].apply(clean_text)\n",
    "# vectorizer = TfidfVectorizer(ngram_range=ngram_range,max_df=0.999, min_df=0.001)\n",
    "# data_tf = vectorizer.fit(data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f696a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f7f62b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = vectorizer.transform(data['text'].tolist())\n",
    "# X_tf_train_df = pd.DataFrame(tf.toarray(),columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bdd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8933a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525f251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144aa11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c3e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    10  100   11   12   15   16   17   20   25   30  ...  your  your advice  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0          0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0          0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0          0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0          0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0          0.0   \n",
      "\n",
      "   your comment  your dad  your input  your life  your service  yours  \\\n",
      "0           0.0       0.0         0.0        0.0           0.0    0.0   \n",
      "1           0.0       0.0         0.0        0.0           0.0    0.0   \n",
      "2           0.0       0.0         0.0        0.0           0.0    0.0   \n",
      "3           0.0       0.0         0.0        0.0           0.0    0.0   \n",
      "4           0.0       0.0         0.0        0.0           0.0    0.0   \n",
      "\n",
      "   yourself  youtube  \n",
      "0       0.0      0.0  \n",
      "1       0.0      0.0  \n",
      "2       0.0      0.0  \n",
      "3       0.0      0.0  \n",
      "4       0.0      0.0  \n",
      "\n",
      "[5 rows x 1913 columns]\n"
     ]
    }
   ],
   "source": [
    "ngram_range=(1,3)\n",
    "max_df = 0.95 \n",
    "mindf =  0.05 \n",
    "# max_features=\n",
    "\n",
    "#vectorizer = TfidfVectorizer()\n",
    "vectorizer = TfidfVectorizer(ngram_range=ngram_range,max_df=0.999, min_df=0.001)\n",
    "vectorizer.fit(X_train['text'].tolist())\n",
    "X_tf_train = vectorizer.transform(X_train['text'].tolist())\n",
    "X_tf_train_df = pd.DataFrame(X_tf_train.toarray(),columns=vectorizer.get_feature_names())\n",
    "X_tf_test = vectorizer.transform(X_test['text'].tolist())\n",
    "X_tf_test_df = pd.DataFrame(X_tf_test.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X_tf_train_df.head())\n",
    "\n",
    "# no_features =1000\n",
    "\n",
    "\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_df=1, min_df=0.05, max_features=no_features, stop_words='english')\n",
    "# tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# CountVec = CountVectorizer(max_features=no_features, stop_words='english')\n",
    "# CV = CountVec.fit_transform(X_train)\n",
    "# CV_feature_names = CountVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "444e4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "    pickle.dump(vectorizer, fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61626de",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719c3c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       0.400574\n",
       "gratitude     0.192153\n",
       "admiration    0.125167\n",
       "love          0.070048\n",
       "amusement     0.059330\n",
       "curiosity     0.038469\n",
       "approval      0.036172\n",
       "anger         0.027177\n",
       "optimism      0.026794\n",
       "sadness       0.024115\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cec7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tf_train_df['subjectivity'] = subjectivity_train_x.reset_index(drop=True)\n",
    "X_tf_train_df['polarity'] = polarity_train_x.reset_index(drop=True)\n",
    "X_tf_train_df['upper_word_dist'] = upper_word_freq_train_x.reset_index(drop=True)\n",
    "X_tf_train_df['punkt_frew'] = punkt_frew_nor_train_x.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "X_tf_test_df['subjectivity'] = subjectivity_test_x.reset_index(drop=True)\n",
    "X_tf_test_df['polarity'] = polarity_test_x.reset_index(drop=True)\n",
    "X_tf_test_df['upper_word_dist'] = upper_word_freq_test_x.reset_index(drop=True)\n",
    "X_tf_test_df['punkt_frew'] = punkt_frew_nor_test_x.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd9e0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nmnnneeeewww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b282dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time is 0.4863574504852295\n",
      "{'max_depth': 60, 'min_samples_leaf': 5}\n",
      "Train score is 0.6522488038277512 and Test score is0.6151491966335119\n",
      "cross_validation score is 0.7666985645933015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  admiration       0.62      0.74      0.67       164\n",
      "   amusement       0.90      0.45      0.60        78\n",
      "       anger       0.14      0.14      0.14        35\n",
      "    approval       0.15      0.17      0.16        47\n",
      "   curiosity       0.16      0.54      0.24        50\n",
      "   gratitude       0.99      0.95      0.97       251\n",
      "        love       0.62      0.92      0.74        92\n",
      "     neutral       0.80      0.47      0.60       523\n",
      "    optimism       0.43      0.66      0.52        35\n",
      "     sadness       0.20      0.41      0.27        32\n",
      "\n",
      "    accuracy                           0.62      1307\n",
      "   macro avg       0.50      0.55      0.49      1307\n",
      "weighted avg       0.72      0.62      0.64      1307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y1=y_train\n",
    "X1=X_tf_train_df  \n",
    "x_test,y_test=X_tf_test_df,y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid1 = GridSearchCV(RandomForestClassifier(random_state=42,class_weight=\"balanced\",n_estimators=100),\n",
    "             param_grid={\"max_depth\":range(10,100,5),\n",
    "                         \"min_samples_leaf\":range(5,40,5) },scoring=\"f1_macro\",cv=5,n_jobs=-1).fit(X1,Y1)\n",
    "\n",
    "params=list(grid1.best_params_.values())\n",
    "\n",
    "max_depth,min_samples_leaf=params\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "forest=RandomForestClassifier(random_state=42,class_weight=\"balanced\",max_depth=max_depth,min_samples_leaf=max_depth).fit(X1,Y1)\n",
    "\n",
    "end_time = time.time()\n",
    "timer = end_time-start_time\n",
    "print(f\"Time is {timer}\")\n",
    "\n",
    "train_score=forest.score(X1,Y1)\n",
    "test_score=forest.score(x_test,y_test)\n",
    "print(grid1.best_params_)\n",
    "print(\"Train score is %s and Test score is%s\"%(train_score,test_score))\n",
    "print(\"cross_validation score is %s\"% (np.mean(cross_val_score(RandomForestClassifier(random_state=42,class_weight=\"balanced\",max_depth=max_depth,min_samples_leaf=min_samples_leaf),X1,Y1,cv=5))))\n",
    "y_pred_train=forest.predict(x_test)\n",
    "y_pred_test=forest.predict(X1)\n",
    "print(classification_report(y_test,forest.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f79e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([X_tf_train_df,X_tf_test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5e3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y = pd.concat([Y1,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2b5f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = all_x.reset_index(drop=True)\n",
    "all_y = all_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa132f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced',\n",
       "                                              random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': range(10, 100, 5),\n",
       "                         'min_samples_leaf': range(5, 40, 5)},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid1.fit(all_x,all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "876f884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'forest.sav'\n",
    "pickle.dump(grid1, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98f936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
